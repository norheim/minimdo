{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1 = [u1 == x1**2 + x2,\n",
    "#       u2 == u1+x3-0.2*u4]\n",
    "# d2 = [u3 == x1+x2,\n",
    "#       u4 == u2**0.5+u3]\n",
    "\n",
    "d1_res = lambda x, u: [u[0] - x[0] - x[1]**2,  u[1] - u[0] - x[2] + 0.2*u[3]]\n",
    "d2_res = lambda x, u: [u[2] - x[0] - x[1],  u[3] - u[1]**0.5 - u[2]]\n",
    "d1_analysis_u1_u2 = lambda x, u: [x[0]+x[1]**2, x[0]+x[1]**2+x[2]-0.2*u[3]]\n",
    "d2_analysis_u3_u4 = lambda x, u: [x[0]+x[1], u[1]**0.5+x[0]+x[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis: has two functions; evaluate(residual) and solve()\n",
    "# eliminate: takes two analyses, one to eliminate and one to eliminate from\n",
    "# analysis(lambda x,y: A(x)*y - b, lambda x: np.linalg.solve(A(x),b)\n",
    "# analysis.solve(x) -> y\n",
    "\n",
    "# vision to integrate with pytorch  / without elimination\n",
    "# def all_analysis.__init__(self, analyses):\n",
    "#   self.analyses = analyses\n",
    "# def all_analysis.forward(self, x):\n",
    "# for analysis in self.analyses:\n",
    "#   x = analysis.forward() \n",
    "# return x\n",
    "\n",
    "# one analysis forward:\n",
    "# def forward(z):\n",
    "#   x,y = z\n",
    "#   return A(x)*y - b\n",
    "\n",
    "# eliminate\n",
    "# def forward(z):\n",
    "#   z[y1idx] = b1 + x[1]\n",
    "#   return A(z)\n",
    "\n",
    "# one analysis forward with solve\n",
    "# def forward(x):\n",
    "#   x[output_idx] = np.linalg.solve(A(x),b) #alternatively, use index_put\n",
    "#   return y\n",
    "\n",
    "# / with elimination\n",
    "# setup of A with elimination\n",
    "# x = Variable()\n",
    "# A1elim.forward() # calculates one value of y\n",
    "# Arest.forward() # calculates the rest of the values of y\n",
    "\n",
    "# x.forward()\n",
    "# Aelim_solve.forward() # solves A(x)*y-b for y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value at x: tensor([1., 4., 9.])\n",
      "Automatic gradient: tensor([2., 4., 6.])\n",
      "Manual gradient: tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LambdaFunctionModule(nn.Module):\n",
    "    def __init__(self, func, grad_func):\n",
    "        super(LambdaFunctionModule, self).__init__()\n",
    "        # Store the lambda function and its gradient\n",
    "        self.func = func\n",
    "        self.grad_func = grad_func\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x requires gradient\n",
    "        if not x.requires_grad:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        # Evaluate the function\n",
    "        y = self.func(x)\n",
    "\n",
    "        # Create an initial gradient for non-scalar output\n",
    "        if y.dim() > 0:  # y is not a scalar\n",
    "            grad_output = torch.ones_like(y)\n",
    "        else:  # y is a scalar\n",
    "            grad_output = None\n",
    "\n",
    "        # Use autograd to compute the gradient\n",
    "        y.backward(gradient=grad_output)  # Computes the gradient of y with respect to x\n",
    "        auto_grad = x.grad.clone()  # Clone to avoid being erased after zeroing gradients\n",
    "\n",
    "        # Reset gradients in x for manual gradient computation\n",
    "        x.grad.data.zero_()\n",
    "\n",
    "        # Compute the manual gradient\n",
    "        manual_grad = self.grad_func(x)\n",
    "\n",
    "        # Ensure manual_grad is in the correct form (tensor)\n",
    "        if not isinstance(manual_grad, torch.Tensor):\n",
    "            manual_grad = torch.tensor(manual_grad, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        # Return the function value, automatic gradient, and manual gradient\n",
    "        return y.detach(), auto_grad, manual_grad\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple function and its gradient\n",
    "    func = lambda x: x ** 2\n",
    "    grad_func = lambda x: 2 * x\n",
    "\n",
    "    # Create a vector of parameters x\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "    # Instantiate the module\n",
    "    module = LambdaFunctionModule(func, grad_func)\n",
    "\n",
    "    # Evaluate the function and gradients\n",
    "    y, auto_grad, manual_grad = module(x)\n",
    "\n",
    "    print(\"Function value at x:\", y)\n",
    "    print(\"Automatic gradient:\", auto_grad)\n",
    "    print(\"Manual gradient:\", manual_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(x):\n",
      "tensor([[2.1667, 0.3333],\n",
      "        [0.3333, 2.6667]])\n",
      "\n",
      "b(x):\n",
      "tensor([2.8333, 5.6667])\n",
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "def generate_dense_A_and_b_diff_sizes(x, n):\n",
    "    # Extend x to match the desired dimensions, n\n",
    "    # Here, we're using a simple repeat and trim strategy, but more complex methods could be used\n",
    "    extended_x = x.repeat((n + len(x) - 1) // len(x))[:n]\n",
    "    \n",
    "    # Generate an initial base matrix, e.g., identity matrix scaled\n",
    "    A_base = torch.eye(n) * (1 + torch.abs(extended_x[0]))\n",
    "    \n",
    "    # Apply variations based on the extended x to make A fully dense\n",
    "    # Using outer product with a small perturbation to ensure it doesn't make A singular\n",
    "    A_variation = torch.ger(extended_x, extended_x) / (torch.norm(extended_x)**2 + 1)\n",
    "    A = A_base + A_variation\n",
    "    \n",
    "    # Generate b(x) as a function of the extended x\n",
    "    # This ensures compatibility with A's dimensions and that there's always a solution\n",
    "    b = torch.matmul(A, extended_x)\n",
    "    \n",
    "    return A, b\n",
    "\n",
    "\n",
    "# Example usage\n",
    "n = 2  # Dimension of the square matrix A\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)  # Example vector x with different size\n",
    "A, b = generate_dense_A_and_b_diff_sizes(x, n)\n",
    "print(\"A(x):\")\n",
    "print(A)\n",
    "print(\"\\nb(x):\")\n",
    "print(b)\n",
    "print(np.linalg.solve(A, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subdivide_tensor(tensor, indices):\n",
    "    # Create a mask for all elements\n",
    "    mask = torch.ones(tensor.size(0), dtype=torch.bool)\n",
    "    # Set the selected indices to False\n",
    "    mask[indices] = False\n",
    "    # Subdivide the tensor\n",
    "    tensor1 = tensor[indices]\n",
    "    tensor2 = tensor[mask]\n",
    "    return tensor1, tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_residual(solveindices): # should return a function with forward? or not necessary\n",
    "    def residual(x):\n",
    "        y, z = subdivide_tensor(x, solveindices) \n",
    "        n = len(y)\n",
    "        A, b = generate_dense_A_and_b_diff_sizes(z, n)\n",
    "        #print(np.linalg.solve(A, b))\n",
    "        return A @ y-b\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual = generate_residual([0, 1, 2])\n",
    "residual(torch.tensor([8, 5, 8, 8.0, 5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class solverModule(nn.Module):\n",
    "    def __init__(self, residual, solveindices):\n",
    "        super(solverModule, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.solveindices = solveindices\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y0 = x[self.solveindices].clone()\n",
    "        def fsolvefunc(y):\n",
    "            x[self.solveindices] = torch.tensor(y, dtype=torch.float32)\n",
    "            return self.residual(x)\n",
    "        out = fsolve(fsolvefunc, y0) # x gets modified in place anyways\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = solverModule(residual, [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 4., 4., 5.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm(torch.tensor([1, 2, 3, 4.0, 5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.arghandling import Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.        , 0.03401518, 0.97972786, 0.8262554 , 0.35029405],            dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoding(('a','b'), (1,), (2,2))\n",
    "E.encode({'a':1, 'b':torch.rand(2,2)}, flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small example\n",
    "![PyTorch MDO (1).png](<residual_example.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = (('x2', 'x3'), ('x1','x4'), lambda a, b: (sum(a)+b, a[:2]+b))\n",
    "e2 = (('x1', 'x3'), ('x5',), lambda a, b: (a@b,))\n",
    "r = (('x4', 'x5'), lambda a, b: torch.cat((a, b)))\n",
    "indices = {\n",
    "    'x1': [0,1],\n",
    "    'x2': [2,3,4],\n",
    "    'x3': [5,6],\n",
    "    'x4': [7,8],\n",
    "    'x5': [9]\n",
    "}\n",
    "# update indices to have tensor entries instead of lists\n",
    "for k,v in indices.items():\n",
    "    indices[k] = torch.tensor(v, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalysisFunction(triplet):\n",
    "    inputs, outputs, function = triplet\n",
    "    def forward(x):\n",
    "        all_inputs = [x[indices[xin]] for xin in inputs]\n",
    "        out = function(*all_inputs)\n",
    "        for i, xout in enumerate(outputs):\n",
    "            x = x.index_put((indices[xout],), out[i])\n",
    "        return x\n",
    "    return forward\n",
    "\n",
    "def Function(tupl):\n",
    "    inputs, function = tupl\n",
    "    def forward(x):\n",
    "        all_inputs = [x[indices[xin]] for xin in inputs]\n",
    "        return function(*all_inputs)\n",
    "    return forward\n",
    "\n",
    "def EliminateAnalysis(analyses, functions):\n",
    "    def forward(x):\n",
    "        for a in analyses:\n",
    "            x = a(x)\n",
    "        return torch.cat([f(x) for f in functions])\n",
    "    return forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 3.], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(10)\n",
    "# Fix parameters (x3)\n",
    "x0[indices['x3']] = torch.tensor([1., 2])\n",
    "# Set initial values for solve vars (x2)\n",
    "x0[indices['x2']] = torch.tensor([-1, -1, 4/3], dtype=torch.float32)\n",
    "xvar = x0.clone()\n",
    "xvar.requires_grad_()\n",
    "A1 = AnalysisFunction(e1)\n",
    "A2 = AnalysisFunction(e2)\n",
    "R = Function(r)\n",
    "S = EliminateAnalysis([A1, A2], [R])\n",
    "S(xvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 1.3333, 3.3333, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(S, xvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "indices = ([0,1], [2,3,4], [5,6], [7,8], [9])\n",
    "indices = [torch.tensor(i, dtype=torch.long) for i in indices]\n",
    "\n",
    "def elim_and_residual(x):\n",
    "    x2, x3 = x[indices[1]], x[indices[2]]\n",
    "    # elimin 1\n",
    "    x = x.index_put((indices[0],), sum(x2)+x3)\n",
    "    x = x.index_put((indices[3],), x2[:2]+x3)\n",
    "    # elimin 2\n",
    "    x1 = x[indices[0]]\n",
    "    x = x.index_put((indices[4],), x1@x3)\n",
    "    # residual\n",
    "    r = torch.cat((x[indices[4]], x[indices[3]]))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 0., 1.], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(10)\n",
    "# Fix parameters (x3)\n",
    "x0[indices[2]] = torch.tensor([1., 2])\n",
    "# Set initial values for solve vars (x2)\n",
    "x0[indices[1]] = torch.tensor([-1, -1, 4/3], dtype=torch.float32)\n",
    "xvar = x0.clone()\n",
    "xvar.requires_grad_()\n",
    "#torch.autograd.functional.jacobian(elim_and_residual, xvar)\n",
    "elim_and_residual(xvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "import numpy as np\n",
    "\n",
    "# some intermediary calculations in sequence\n",
    "\n",
    "# then need to have a module with a forward and backward (that will be used for calculating the jacobian of the final functions later)\n",
    "class ElimResidualFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, function, solvefor, inputs, x):\n",
    "        def eval_and_gradient():\n",
    "            jacobian = None\n",
    "            def eval_function(y):\n",
    "                nonlocal jacobian\n",
    "                x.data[solvefor] = torch.tensor(y, dtype=torch.float32)\n",
    "                r = function(x)\n",
    "                jacobian = torch.autograd.functional.jacobian(function, xvar)\n",
    "                result = r.detach().numpy()\n",
    "                return result\n",
    "\n",
    "            def recover_gradient(y=None):\n",
    "                J = jacobian[:, solvefor]\n",
    "                return J\n",
    "            \n",
    "            def recover_jacobian(y=None):\n",
    "                return jacobian\n",
    "            \n",
    "            return eval_function, recover_gradient, recover_jacobian\n",
    "        \n",
    "        eval_function, recover_gradient, recover_jacobian = eval_and_gradient()\n",
    "        xguess = x.data[solvefor] \n",
    "        fsolve(eval_function, xguess, fprime=recover_gradient) #sets x in place\n",
    "        J = recover_jacobian()\n",
    "        ctx.save_for_backward(J, solvefor, inputs)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        J, solvefor, inputs = ctx.saved_tensors\n",
    "        J_u = J[:, solvefor]\n",
    "        J_x = J[:, inputs]\n",
    "        dudx = np.linalg.solve(-J_u, J_x)\n",
    "        result = torch.zeros_like(grad_output)\n",
    "        result[inputs] = grad_output[solvefor] @ dudx\n",
    "        return None, None, None, result\n",
    "    \n",
    "class ElimResidual(torch.nn.Module):\n",
    "    def __init__(self, function, solvefor, inputs):\n",
    "        super(ElimResidual, self).__init__()\n",
    "        self.function = function\n",
    "        self.solvefor = solvefor\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return ElimResidualFunc.apply(self.function, self.solvefor, self.inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = ElimResidual(elim_and_residual, indices[1], indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.rand(10)\n",
    "# Fix parameters (x3)\n",
    "x0[indices[2]] = torch.tensor([1., 2])\n",
    "# Set initial values for solve vars (x2)\n",
    "x0[indices[1]] = torch.tensor([-1, -1, 4/3], dtype=torch.float32)\n",
    "xvar = x0.clone()\n",
    "xvar.requires_grad_()\n",
    "xout = er(xvar)\n",
    "backpass = torch.zeros_like(xvar)\n",
    "backpass[4] = 1\n",
    "xout.backward(backpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8903, 0.2236, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvar.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimdo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
